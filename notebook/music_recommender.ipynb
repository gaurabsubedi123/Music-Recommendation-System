{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import keras.backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    \n",
    "    def __init__(self, datapath, itempath):\n",
    "        \n",
    "        '''\n",
    "        Load data \n",
    "        List the users and the music items\n",
    "        List all the users historic\n",
    "        '''\n",
    "        \n",
    "        self.data  = self.load_datas(datapath, itempath)\n",
    "        self.users = self.data['session_id'].unique()  #list of all users\n",
    "        self.histo = self.gen_histo()\n",
    "        self.train = []\n",
    "        self.test  = []\n",
    "\n",
    "    def load_music_data(self, itempath):\n",
    "        \n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        datapath :  string\n",
    "                    path to the music data\n",
    "        '''\n",
    "        \n",
    "        music = pd.read_csv(itempath, compression = 'gzip')\n",
    "        music = music.rename(columns = {music.columns[0]:'track_id'})\n",
    "        music = music[music.us_popularity_estimate>=95] # Focus on the song tracks that have good popularity\n",
    "\n",
    "        return music\n",
    "    \n",
    "    def load_user_data(self, datapath):\n",
    "        \n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        datapath :  string\n",
    "                    path to the session data\n",
    "        '''\n",
    "        \n",
    "        user = pd.read_csv(datapath, compression = 'gzip', nrows = 100000)\n",
    "        user = user.rename(columns = {user.columns[0]:'session_id'})\n",
    "        \n",
    "        # Correct the labeling of skip\n",
    "        # rating = 1 if user doesn't the skip the music, otherwise rating = 0\n",
    "        user['rating'] = user.skip_2.astype(int)*-1+1\n",
    "        \n",
    "        return user\n",
    "    \n",
    "    def load_datas(self, datapath, itempath):\n",
    "        \n",
    "        '''\n",
    "        Load the data and merge the id of each song track. \n",
    "        A row corresponds to a rate given by a user to a song.\n",
    "        '''\n",
    "        \n",
    "        music = self.load_music_data(itempath)\n",
    "        data = self.load_user_data(datapath)\n",
    "        data = pd.merge(data, music, how = 'inner', left_on = ['track_id_clean'], right_on = ['track_id'])\n",
    "        data = data.groupby(['session_id','session_length']).filter(lambda x: len(x) == 20) # Filter out the sessions that are too short\n",
    "        data['session_id'] = LabelEncoder().fit_transform(data['session_id'])\n",
    "        data['music_id'] = LabelEncoder().fit_transform(data['track_id'])\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def gen_histo(self):\n",
    "        \n",
    "        '''\n",
    "        Group all rates given by users and store them from older to most recent.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        result :    List(DataFrame)\n",
    "                    List of the historic for each user\n",
    "        '''\n",
    "        \n",
    "        historic_users = []\n",
    "        for i, u in enumerate(self.users):\n",
    "            temp = self.data[self.data['session_id'] == u]\n",
    "            temp = temp.sort_values('session_position').reset_index()\n",
    "            temp.drop('index', axis = 1, inplace = True)\n",
    "            historic_users.append(temp)\n",
    "        return historic_users\n",
    "    \n",
    "    def sample_histo(self, user_histo, action_ratio = 0.8, max_samp_by_user = 10,  max_state = 10, max_action = 5,\n",
    "                 nb_states = [], nb_actions = []):\n",
    "        \n",
    "        '''\n",
    "        For a given historic, make one or multiple sampling.\n",
    "        If no optional argument given for nb_states and nb_actions, then the sampling\n",
    "        is random and each sample can have differents size for action and state.\n",
    "        To normalize sampling we need to give list of the numbers of states and actions\n",
    "        to be sampled.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_histo :  DataFrame\n",
    "                          historic of user\n",
    "        delimiter :       string, optional\n",
    "                          delimiter for the csv\n",
    "        action_ratio :    float, optional\n",
    "                          ratio form which song tracks in history will be selected\n",
    "        max_samp_by_user: int, optional\n",
    "                          Nulber max of sample to make by user\n",
    "        max_state :       int, optional\n",
    "                          Number max of song tracks to take for the 'state' column\n",
    "        max_action :      int, optional\n",
    "                          Number max of song tracks to take for the 'action' action\n",
    "        nb_states :       array(int), optional\n",
    "                          Numbers of song tracks to be taken for each sample made on user's historic\n",
    "        nb_actions :      array(int), optional\n",
    "                          Numbers of rating to be taken for each sample made on user's historic\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        states :         List(String)\n",
    "                        All the states sampled, format of a sample: itemId&rating\n",
    "        actions :        List(String)\n",
    "                        All the actions sampled, format of a sample: itemId&rating\n",
    "      \n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        States must be before(timestamp<) the actions.\n",
    "        If given, size of nb_states is the numbller of sample by user\n",
    "        sizes of nb_states and nb_actions must be equals\n",
    "        '''\n",
    "\n",
    "        n = len(user_histo)\n",
    "        sep = int(action_ratio * n)\n",
    "        nb_sample = random.randint(1, max_samp_by_user)\n",
    "        if not nb_states:\n",
    "            nb_states = [min(random.randint(1, sep), max_state) for i in range(nb_sample)]\n",
    "        if not nb_actions:\n",
    "            nb_actions = [min(random.randint(1, n - sep), max_action) for i in range(nb_sample)]\n",
    "        assert len(nb_states) == len(nb_actions), 'Given array must have the same size'\n",
    "        \n",
    "        states  = []\n",
    "        actions = []\n",
    "        # Select samples in histo \n",
    "        for i in range(len(nb_states)):\n",
    "            sample_states = user_histo.iloc[0:sep].sample(nb_states[i])\n",
    "            sample_actions = user_histo.iloc[-(n - sep):].sample(nb_actions[i])\n",
    "            \n",
    "            sample_state  = []\n",
    "            sample_action = []\n",
    "            for j in range(nb_states[i]):\n",
    "                row = sample_states.iloc[j]\n",
    "                # Formate State\n",
    "                state = str(row.loc['music_id'])+'&'+str(row.loc['rating'].astype(int)) #TODO: REWARD SHAPING\n",
    "                sample_state.append(state)\n",
    "          \n",
    "            for j in range(nb_actions[i]):\n",
    "                row = sample_actions.iloc[j]\n",
    "                # Formate Action \n",
    "                action = str(row.loc['music_id'])+'&'+str(row.loc['rating'].astype(int))\n",
    "                sample_action.append(action)\n",
    "\n",
    "            states.append(sample_state)\n",
    "            actions.append(sample_action)\n",
    "                \n",
    "        return states, actions\n",
    "    \n",
    "    def gen_train_test(self, train_ratio, seed = None):\n",
    "        \n",
    "        '''\n",
    "        Shuffle the historic of users and separate it in a train and a test set.\n",
    "        Store the ids for each set.\n",
    "        An user can't be in both set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_ratio :  float\n",
    "                      Ratio to control the sizes of the sets\n",
    "        seed       :  float\n",
    "                      Seed on the shuffle\n",
    "        '''\n",
    "        \n",
    "        n = len(self.histo)\n",
    "\n",
    "        if seed is not None:\n",
    "            random.Random(seed).shuffle(self.histo)\n",
    "        else:\n",
    "            random.shuffle(self.histo)\n",
    "\n",
    "        self.train = self.histo[:int((train_ratio * n))]\n",
    "        self.test  = self.histo[int((train_ratio * n)):]\n",
    "        self.user_train = [h.iloc[0,0] for h in self.train]\n",
    "        self.user_test  = [h.iloc[0,0] for h in self.test]\n",
    "        \n",
    "    def write_csv(self, filename, histo_to_write, delimiter = ';', action_ratio = 0.8, max_samp_by_user = 10,\n",
    "                  max_state = 10, max_action = 5, nb_states = [], nb_actions = []):\n",
    "        \n",
    "        '''\n",
    "        From  a given historic, create a csv file with the format:\n",
    "        columns : state;action_reward;n_state\n",
    "        rows    : itemid&rating1 | itemid&rating2 | ... ; itemid&rating3 | ...  | ... | item&rating4\n",
    "        at filename location.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename :        string\n",
    "                          path to the file to be produced\n",
    "        histo_to_write :  List(DataFrame)\n",
    "                          List of the historic for each user\n",
    "        delimiter :       string, optional\n",
    "                          delimiter for the csv\n",
    "        action_ratio :    float, optional\n",
    "                          ratio form which song tracks in history will be selected\n",
    "        max_samp_by_user: int, optional\n",
    "                          Nulber max of sample to make by user\n",
    "        max_state :       int, optional\n",
    "                          Number max of song tracks to take for the 'state' column\n",
    "        max_action :      int, optional\n",
    "                          Number max of song tracks to take for the 'action' action\n",
    "        nb_states :       array(int), optional\n",
    "                          Numbers of song tracks to be taken for each sample made on user's historic\n",
    "        nb_actions :      array(int), optional\n",
    "                          Numbers of rating to be taken for each sample made on user's historic\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        if given, size of nb_states is the numbller of sample by user\n",
    "        sizes of nb_states and nb_actions must be equals\n",
    "\n",
    "        '''\n",
    "        \n",
    "        with open(filename, mode = 'w') as file:\n",
    "            f_writer = csv.writer(file, delimiter = delimiter)\n",
    "            f_writer.writerow(['state', 'action_reward', 'n_state'])\n",
    "            for user_histo in histo_to_write:\n",
    "                states, actions = self.sample_histo(user_histo, action_ratio, max_samp_by_user, max_state, max_action, nb_states, nb_actions)\n",
    "                for i in range(len(states)):\n",
    "                    # FORMAT STATE\n",
    "                    state_str   = '|'.join(states[i])\n",
    "                    # FORMAT ACTION\n",
    "                    action_str  = '|'.join(actions[i])\n",
    "                    # FORMAT N_STATE\n",
    "                    n_state_str = state_str + '|' + action_str\n",
    "                    f_writer.writerow([state_str, action_str, n_state_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings():\n",
    "\n",
    "    '''\n",
    "    Returns Embeddings and embedding metadata after initialization with item_embeddings\n",
    "    '''\n",
    "\n",
    "    def __init__(self, item_embeddings):\n",
    "        self.item_embeddings = item_embeddings\n",
    "    \n",
    "    def size(self):\n",
    "        return self.item_embeddings.shape[1]\n",
    "    \n",
    "    def get_embedding_vector(self):\n",
    "        return self.item_embeddings\n",
    "    \n",
    "    def get_embedding(self, item_index):\n",
    "        return self.item_embeddings[item_index]\n",
    "\n",
    "    def embed(self, item_list):\n",
    "        return np.array([self.get_embedding(item) for item in item_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsGenerator():\n",
    "\n",
    "    '''\n",
    "    Returns Embeddings after initialization with train_users and data\n",
    "    '''\n",
    "    \n",
    "    def  __init__(self, train_users, data):\n",
    "        \n",
    "        self.train_users = train_users\n",
    "        self.data = data.sort_values(by=['date'])\n",
    "        self.session_count = self.data['session_id'].max()+1\n",
    "        self.track_count = self.data['music_id'].max()+1\n",
    "        self.session_tracks = {} # list of rated song tracks by each session\n",
    "        for sessionId in range(self.session_count):\n",
    "            self.session_tracks[sessionId] = self.data[self.data.session_id == sessionId]['music_id'].tolist()\n",
    "        self.m = self.model()\n",
    "\n",
    "    def model(self, hidden_layer_size = 300):\n",
    "        \n",
    "        m = Sequential()\n",
    "        m.add(Dense(hidden_layer_size, input_shape = (1, self.track_count)))\n",
    "        m.add(Dense(self.track_count, activation = 'softmax'))\n",
    "        m.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        return m\n",
    "    \n",
    "    def generate_input(self, session_id):\n",
    "        \n",
    "        '''\n",
    "        Returns a context and a target for the user_id\n",
    "        context: user's history with one random song removed\n",
    "        target: id of random removed song\n",
    "        '''\n",
    "        \n",
    "        session_tracks_count = len(self.session_tracks[session_id])\n",
    "        # picking random song\n",
    "        random_index = np.random.randint(0, session_tracks_count-1) # -1 avoids taking the last track\n",
    "        # setting target\n",
    "        target = np.zeros((1, self.track_count))\n",
    "        target[0][self.session_tracks[session_id][random_index]] = 1\n",
    "        # setting context\n",
    "        context = np.zeros((1, self.track_count))\n",
    "        context[0][self.session_tracks[session_id][:random_index] + self.session_tracks[session_id][random_index+1:]] = 1\n",
    "        return context, target\n",
    "\n",
    "    def train(self, nb_epochs, batch_size = 2000):\n",
    "        \n",
    "        '''\n",
    "        Trains the model from train_users's history\n",
    "        '''\n",
    "        \n",
    "        for i in range(nb_epochs):\n",
    "            print('%d/%d' % (i+1, nb_epochs))\n",
    "            batch = [self.generate_input(session_id = np.random.choice(self.train_users)) for _ in range(batch_size)]\n",
    "            X_train = np.array([b[0] for b in batch])\n",
    "            y_train = np.array([b[1] for b in batch])\n",
    "            self.m.fit(X_train, y_train, epochs = 1, validation_split = 0.3)\n",
    "\n",
    "    def test(self, test_users, batch_size = 2000):\n",
    "        \n",
    "        '''\n",
    "        Returns [loss, accuracy] on the test set\n",
    "        '''\n",
    "        \n",
    "        print('test users', len(test_users))\n",
    "        batch_test = [self.generate_input(session_id = np.random.choice(test_users)) for _ in range(batch_size)]\n",
    "        X_test = np.array([b[0] for b in batch_test])\n",
    "        y_test = np.array([b[1] for b in batch_test])\n",
    "        return self.m.evaluate(X_test, y_test)\n",
    "\n",
    "    def save_embeddings(self, file_name):\n",
    "        \n",
    "        '''\n",
    "        Generates a csv file containg the vector embedding for each song\n",
    "        '''\n",
    "        \n",
    "        inp = self.m.input                                           # input placeholder\n",
    "        outputs = [layer.output for layer in self.m.layers]          # all layer outputs\n",
    "        functor = K.function([inp, K.learning_phase()], outputs )    # evaluation function\n",
    "\n",
    "        # append embeddings to vectors\n",
    "        vectors = []\n",
    "        for music_id in range(self.track_count):\n",
    "            track = np.zeros((1, 1, self.track_count))\n",
    "            track[0][0][music_id] = 1\n",
    "            layer_outs = functor([track])\n",
    "            vector = [str(v) for v in layer_outs[0][0][0]]\n",
    "            vector = '|'.join(vector)\n",
    "            vectors.append([music_id, vector])\n",
    "\n",
    "        #saves as a csv file\n",
    "        embeddings = pd.DataFrame(vectors, columns = ['music_id', 'vectors']).astype({'music_id': 'int32'})\n",
    "        embeddings.to_csv(file_name, sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data_path):\n",
    "    \n",
    "    '''\n",
    "    Load data from train_set.csv or test_set.csv\n",
    "    '''\n",
    "\n",
    "    data = pd.read_csv(data_path, sep = ';')\n",
    "    for col in ['state', 'n_state', 'action_reward']:\n",
    "        data[col] = [np.array([[np.int(float(k)) for k in ee.split('&')] for ee in e.split('|')]) for e in data[col]]\n",
    "    for col in ['state', 'n_state']:\n",
    "        data[col] = [np.array([e[0] for e in l]) for l in data[col]]\n",
    "\n",
    "    data['action'] = [[e[0] for e in l] for l in data['action_reward']]\n",
    "    data['reward'] = [tuple(e[1] for e in l) for l in data['action_reward']]\n",
    "    data.drop(columns = ['action_reward'], inplace = True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def read_embeddings(embeddings_path):\n",
    "    \n",
    "    '''\n",
    "    Load embeddings (a vector for each item)\n",
    "    '''\n",
    "    \n",
    "    embeddings = pd.read_csv(embeddings_path, sep = ';')\n",
    "\n",
    "    return np.array([[np.float64(k) for k in e.split('|')] for e in embeddings['vectors']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self, data, embeddings, alpha, gamma, fixed_length):\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        self.embedded_data = pd.DataFrame()\n",
    "        self.embedded_data['state'] = [np.array([embeddings.get_embedding(item_id) \n",
    "                                                 for item_id in row['state']]) for _, row in data.iterrows()]\n",
    "        self.embedded_data['action'] = [np.array([embeddings.get_embedding(item_id) \n",
    "                                                  for item_id in row['action']]) for _, row in data.iterrows()]\n",
    "        self.embedded_data['reward'] = data['reward']\n",
    "\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "        self.fixed_length = fixed_length\n",
    "        self.current_state = self.reset()\n",
    "        self.groups = self.get_groups()\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.init_state = self.embedded_data['state'].sample(1).values[0]\n",
    "        return self.init_state\n",
    "\n",
    "    def step(self, actions):\n",
    "        \n",
    "        '''\n",
    "        Compute reward and update state.\n",
    "        Args:\n",
    "          actions: embedded chosen items.\n",
    "        Returns:\n",
    "          cumulated_reward: overall reward.\n",
    "          current_state: updated state.\n",
    "        '''\n",
    "\n",
    "        # Compute overall reward \n",
    "        simulated_rewards, cumulated_reward = self.simulate_rewards(self.current_state.reshape((1, -1)), actions.reshape((1, -1)))\n",
    "\n",
    "        for k in range(len(simulated_rewards)): \n",
    "            if simulated_rewards[k] > 0:\n",
    "                self.current_state = np.append(self.current_state, [actions[k]], axis = 0)\n",
    "                if self.fixed_length: \n",
    "                    self.current_state = np.delete(self.current_state, 0, axis = 0)\n",
    "\n",
    "        return cumulated_reward, self.current_state\n",
    "\n",
    "    def get_groups(self):\n",
    "        \n",
    "        '''\n",
    "        Calculate average state/action value for each group\n",
    "        '''\n",
    "\n",
    "        groups = []\n",
    "        for rewards, group in self.embedded_data.groupby(['reward']):\n",
    "            size = group.shape[0]\n",
    "            states = np.array(list(group['state'].values))\n",
    "            actions = np.array(list(group['action'].values))\n",
    "            groups.append({\n",
    "                'size': size,\n",
    "                'rewards': rewards, \n",
    "                'average state': (np.sum(states / np.linalg.norm(states, 2, axis = 1)[:, np.newaxis], axis = 0) / size).reshape((1, -1)), # s_x^-\n",
    "                'average action': (np.sum(actions / np.linalg.norm(actions, 2, axis = 1)[:, np.newaxis], axis = 0) / size).reshape((1, -1)) # a_x^-\n",
    "              })\n",
    "        return groups\n",
    "\n",
    "    def simulate_rewards(self, current_state, chosen_actions, reward_type = 'grouped cosine'):\n",
    "        \n",
    "        '''\n",
    "        Calculate simulated rewards.\n",
    "        Args:\n",
    "          current_state: history, list of embedded items.\n",
    "          chosen_actions: embedded chosen items.\n",
    "          reward_type: from ['normal', 'grouped average', 'grouped cosine'].\n",
    "        Returns:\n",
    "          returned_rewards: most probable rewards.\n",
    "          cumulated_reward: probability weighted rewards.\n",
    "        '''\n",
    "\n",
    "        def cosine_state_action(s_t, a_t, s_i, a_i):\n",
    "            cosine_state = np.dot(s_t, s_i.T) / (np.linalg.norm(s_t, 2) * np.linalg.norm(s_i, 2))\n",
    "            cosine_action = np.dot(a_t, a_i.T) / (np.linalg.norm(a_t, 2) * np.linalg.norm(a_i, 2))\n",
    "            return (self.alpha * cosine_state + (1 - self.alpha) * cosine_action).reshape((1,))\n",
    "\n",
    "        if reward_type == 'normal':\n",
    "            probabilities = [cosine_state_action(current_state, chosen_actions, row['state'], row['action'])\n",
    "                           for _, row in self.embedded_data.iterrows()]\n",
    "        elif reward_type == 'grouped average':\n",
    "            probabilities = np.array([g['size'] for g in self.groups]) *\\\n",
    "            [(self.alpha * (np.dot(current_state, g['average state'].T) / np.linalg.norm(current_state, 2))\\\n",
    "            + (1 - self.alpha) * (np.dot(chosen_actions, g['average action'].T) / np.linalg.norm(chosen_actions, 2)))\n",
    "             for g in self.groups]\n",
    "        elif reward_type == 'grouped cosine':\n",
    "            probabilities = [cosine_state_action(current_state, chosen_actions, g['average state'], g['average action']) \n",
    "                           for g in self.groups]\n",
    "\n",
    "        # Normalize \n",
    "        probabilities = np.array(probabilities) / sum(probabilities)\n",
    "\n",
    "        # Get most probable rewards\n",
    "        if reward_type == 'normal':\n",
    "            returned_rewards = self.embedded_data.iloc[np.argmax(probabilities)]['reward']\n",
    "        elif reward_type in ['grouped average', 'grouped cosine']:\n",
    "            returned_rewards = self.groups[np.argmax(probabilities)]['rewards']\n",
    "\n",
    "        def overall_reward(rewards, gamma):\n",
    "            return np.sum([gamma**k * reward for k, reward in enumerate(rewards)])\n",
    "\n",
    "        if reward_type in ['normal', 'grouped average']:\n",
    "            cumulated_reward = overall_reward(returned_rewards, self.gamma)\n",
    "        elif reward_type == 'grouped cosine':\n",
    "            # Get probability weighted cumulated reward\n",
    "            cumulated_reward = np.sum([p * overall_reward(g['rewards'], self.gamma) for p, g in zip(probabilities, self.groups)])\n",
    "\n",
    "        return returned_rewards, cumulated_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    \n",
    "    '''\n",
    "    Policy function approximator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, sess, state_space_size, action_space_size, batch_size, ra_length, history_length, embedding_size, tau, learning_rate, scope = 'actor'):\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.batch_size = batch_size\n",
    "        self.ra_length = ra_length\n",
    "        self.history_length = history_length\n",
    "        self.embedding_size = embedding_size\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope = scope\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # Build Actor network\n",
    "            self.action_weights, self.state, self.sequence_length = self._build_net('estimator_actor')\n",
    "            self.network_params = tf.trainable_variables()\n",
    "\n",
    "            # Build target Actor network\n",
    "            self.target_action_weights, self.target_state, self.target_sequence_length = self._build_net('target_actor')\n",
    "            self.target_network_params = tf.trainable_variables()[len(self.network_params):] # TODO: why sublist [len(x):]? Maybe because its equal to network_params + target_network_params\n",
    "\n",
    "            # Initialize target network weights with network weights\n",
    "            self.init_target_network_params = [self.target_network_params[i].assign(self.network_params[i]) \n",
    "                                               for i in range(len(self.target_network_params))]\n",
    "\n",
    "            # Update target network weights \n",
    "            self.update_target_network_params = [self.target_network_params[i].assign(\n",
    "            tf.multiply(self.tau, self.network_params[i]) +\n",
    "            tf.multiply(1 - self.tau, self.target_network_params[i]))for i in range(len(self.target_network_params))]\n",
    "\n",
    "            # Gradient computation from Critic's action_gradients\n",
    "            self.action_gradients = tf.placeholder(tf.float32, [None, self.action_space_size])\n",
    "            gradients = tf.gradients(tf.reshape(self.action_weights, [self.batch_size, self.action_space_size], name = '42'),\n",
    "                                     self.network_params, self.action_gradients)\n",
    "            params_gradients = list(map(lambda x: tf.div(x, self.batch_size * self.action_space_size), gradients))\n",
    "\n",
    "            # Compute ∇_a.Q(s, a|θ^µ).∇_θ^π.f_θ^π(s)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(zip(params_gradients, self.network_params))\n",
    "\n",
    "    def _build_net(self, scope):\n",
    "        \n",
    "        '''\n",
    "        Build the (target) Actor network\n",
    "        '''\n",
    "\n",
    "        def gather_last_output(data, seq_lens):\n",
    "            def cli_value(x, v):\n",
    "                y = tf.constant(v, shape = x.get_shape(), dtype = tf.int64)\n",
    "                x = tf.cast(x, tf.int64)\n",
    "                return tf.where(tf.greater(x, y), x, y)\n",
    "\n",
    "            batch_range = tf.range(tf.cast(tf.shape(data)[0], dtype = tf.int64), dtype = tf.int64)\n",
    "            tmp_end = tf.map_fn(lambda x: cli_value(x, 0), seq_lens - 1, dtype = tf.int64)\n",
    "            indices = tf.stack([batch_range, tmp_end], axis = 1)\n",
    "            return tf.gather_nd(data, indices)\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            # Inputs: current state, sequence_length\n",
    "            # Outputs: action weights \n",
    "            state = tf.placeholder(tf.float32, [None, self.state_space_size], 'state')\n",
    "            state_ = tf.reshape(state, [-1, self.history_length, self.embedding_size])\n",
    "            sequence_length = tf.placeholder(tf.int32, [None], 'sequence_length')\n",
    "            cell = tf.nn.rnn_cell.GRUCell(self.embedding_size,\n",
    "                                        activation = tf.nn.relu,\n",
    "                                        kernel_initializer = tf.initializers.random_normal(),\n",
    "                                        bias_initializer = tf.zeros_initializer())\n",
    "            outputs, _ = tf.nn.dynamic_rnn(cell, state_, dtype = tf.float32, sequence_length = sequence_length)\n",
    "            last_output = gather_last_output(outputs, sequence_length) # TODO: replace by h\n",
    "            x = tf.keras.layers.Dense(self.ra_length * self.embedding_size)(last_output)\n",
    "            action_weights = tf.reshape(x, [-1, self.ra_length, self.embedding_size])\n",
    "\n",
    "        return action_weights, state, sequence_length\n",
    "\n",
    "    def train(self, state, sequence_length, action_gradients):\n",
    "        \n",
    "        '''\n",
    "        Compute ∇_a.Q(s, a|θ^µ).∇_θ^π.f_θ^π(s)\n",
    "        '''\n",
    "        \n",
    "        self.sess.run(self.optimizer,\n",
    "                      feed_dict = {\n",
    "                          self.state: state,\n",
    "                          self.sequence_length: sequence_length,\n",
    "                          self.action_gradients: action_gradients})\n",
    "\n",
    "    def predict(self, state, sequence_length):\n",
    "        \n",
    "        return self.sess.run(self.action_weights,\n",
    "                            feed_dict = {\n",
    "                                self.state: state,\n",
    "                                self.sequence_length: sequence_length})\n",
    "\n",
    "    def predict_target(self, state, sequence_length):\n",
    "        \n",
    "        return self.sess.run(self.target_action_weights,\n",
    "                            feed_dict = {\n",
    "                                self.target_state: state,\n",
    "                                self.target_sequence_length: sequence_length})\n",
    "\n",
    "    def init_target_network(self):\n",
    "        \n",
    "        self.sess.run(self.init_target_network_params)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \n",
    "        self.sess.run(self.update_target_network_params)\n",
    "      \n",
    "    def get_recommendation_list(self, ra_length, noisy_state, embeddings, target = False):\n",
    "        \n",
    "        '''\n",
    "        Args:\n",
    "          ra_length: length of the recommendation list.\n",
    "          noisy_state: current/remembered environment state with noise.\n",
    "          embeddings: Embeddings object.\n",
    "          target: boolean to use Actor's network or target network.\n",
    "        Returns:\n",
    "          Recommendation List: list of embedded items as future actions.\n",
    "        '''\n",
    "\n",
    "        def get_score(weights, embedding, batch_size):\n",
    "            \n",
    "            '''\n",
    "            Args:\n",
    "            weights: w_t^k shape = (embedding_size,).\n",
    "            embedding: e_i shape = (embedding_size,).\n",
    "            Returns:\n",
    "            score of the item i: score_i = w_t^k.e_i^T shape = (1,).\n",
    "            '''\n",
    "\n",
    "            ret = np.dot(weights, embedding.T)\n",
    "            return ret\n",
    "\n",
    "        batch_size = noisy_state.shape[0]\n",
    "\n",
    "        # Generate w_t = {w_t^1, ..., w_t^K}\n",
    "        method = self.predict_target if target else self.predict\n",
    "        weights = method(noisy_state, [ra_length] * batch_size)\n",
    "\n",
    "        # Score items\n",
    "        scores = np.array([[[get_score(weights[i][k], embedding, batch_size)\n",
    "                             for embedding in embeddings.get_embedding_vector()]\n",
    "                            for k in range(ra_length)] for i in range(batch_size)])\n",
    "\n",
    "        # return a_t\n",
    "        return np.array([[embeddings.get_embedding(np.argmax(scores[i][k]))\n",
    "                          for k in range(ra_length)] for i in range(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    \n",
    "    '''\n",
    "    Value function approximator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, sess, state_space_size, action_space_size, history_length, embedding_size, tau, learning_rate, scope = 'critic'):\n",
    "        self.sess = sess\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.history_length = history_length\n",
    "        self.embedding_size = embedding_size\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope = scope\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # Build Critic network\n",
    "            self.critic_Q_value, self.state, self.action, self.sequence_length = self._build_net('estimator_critic')\n",
    "            self.network_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'estimator_critic')\n",
    "\n",
    "            # Build target Critic network\n",
    "            self.target_Q_value, self.target_state, self.target_action, self.target_sequence_length = self._build_net('target_critic')\n",
    "            self.target_network_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'target_critic')\n",
    "\n",
    "            # Initialize target network weights with network weights (θ^µ′ ← θ^µ)\n",
    "            self.init_target_network_params = [self.target_network_params[i].assign(self.network_params[i])\n",
    "            for i in range(len(self.target_network_params))]\n",
    "\n",
    "            # Update target network weights (θ^µ′ ← τθ^µ + (1 − τ)θ^µ′)\n",
    "            self.update_target_network_params = [self.target_network_params[i].assign(\n",
    "            tf.multiply(self.tau, self.network_params[i]) +\n",
    "            tf.multiply(1 - self.tau, self.target_network_params[i]))\n",
    "            for i in range(len(self.target_network_params))]\n",
    "\n",
    "            # Minimize MSE between Critic's and target Critic's outputed Q-values\n",
    "            self.expected_reward = tf.placeholder(tf.float32, [None, 1])\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.expected_reward, self.critic_Q_value))\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            # Compute ∇_a.Q(s, a|θ^µ)\n",
    "            self.action_gradients = tf.gradients(self.critic_Q_value, self.action)\n",
    "\n",
    "    def _build_net(self, scope):\n",
    "\n",
    "        '''\n",
    "        Build the (target) Critic network\n",
    "        '''\n",
    "\n",
    "        def gather_last_output(data, seq_lens):\n",
    "            def cli_value(x, v):\n",
    "                y = tf.constant(v, shape = x.get_shape(), dtype = tf.int64)\n",
    "                return tf.where(tf.greater(x, y), x, y)\n",
    "\n",
    "            this_range = tf.range(tf.cast(tf.shape(seq_lens)[0], dtype = tf.int64), dtype = tf.int64)\n",
    "            tmp_end = tf.map_fn(lambda x: cli_value(x, 0), seq_lens - 1, dtype = tf.int64)\n",
    "            indices = tf.stack([this_range, tmp_end], axis = 1)\n",
    "            return tf.gather_nd(data, indices)\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            # Inputs: current state, current action\n",
    "            # Outputs: predicted Q-value\n",
    "            state = tf.placeholder(tf.float32, [None, self.state_space_size], 'state')\n",
    "            state_ = tf.reshape(state, [-1, self.history_length, self.embedding_size])\n",
    "            action = tf.placeholder(tf.float32, [None, self.action_space_size], 'action')\n",
    "            sequence_length = tf.placeholder(tf.int64, [None], name = 'critic_sequence_length')\n",
    "            cell = tf.nn.rnn_cell.GRUCell(self.history_length,\n",
    "                                        activation = tf.nn.relu,\n",
    "                                        kernel_initializer = tf.initializers.random_normal(),\n",
    "                                        bias_initializer = tf.zeros_initializer())\n",
    "            predicted_state, _ = tf.nn.dynamic_rnn(cell, state_, dtype = tf.float32, sequence_length = sequence_length)\n",
    "            predicted_state = gather_last_output(predicted_state, sequence_length)\n",
    "\n",
    "            inputs = tf.concat([predicted_state, action], axis = -1)\n",
    "            layer1 = tf.layers.Dense(32, activation = tf.nn.relu)(inputs)\n",
    "            layer2 = tf.layers.Dense(16, activation = tf.nn.relu)(layer1)\n",
    "            critic_Q_value = tf.layers.Dense(1)(layer2)\n",
    "            return critic_Q_value, state, action, sequence_length\n",
    "\n",
    "    def train(self, state, action, sequence_length, expected_reward):\n",
    "        \n",
    "        '''\n",
    "        Minimize MSE between expected reward and target Critic's Q-value\n",
    "        '''\n",
    "        \n",
    "        return self.sess.run([self.critic_Q_value, self.loss, self.optimizer],\n",
    "                            feed_dict = {\n",
    "                                self.state: state,\n",
    "                                self.action: action,\n",
    "                                self.sequence_length: sequence_length,\n",
    "                                self.expected_reward: expected_reward})\n",
    "\n",
    "    def predict(self, state, action, sequence_length):\n",
    "        \n",
    "        '''\n",
    "        Returns Critic's predicted Q-value\n",
    "        '''\n",
    "        \n",
    "        return self.sess.run(self.critic_Q_value,\n",
    "                            feed_dict = {\n",
    "                                self.state: state,\n",
    "                                self.action: action,\n",
    "                                self.sequence_length: sequence_length})\n",
    "\n",
    "    def predict_target(self, state, action, sequence_length):\n",
    "        \n",
    "        '''\n",
    "        Returns target Critic's predicted Q-value\n",
    "        '''\n",
    "        \n",
    "        return self.sess.run(self.target_Q_value,\n",
    "                            feed_dict = {\n",
    "                                self.target_state: state,\n",
    "                                self.target_action: action,\n",
    "                                self.target_sequence_length: sequence_length})\n",
    "\n",
    "    def get_action_gradients(self, state, action, sequence_length):\n",
    "        \n",
    "        '''\n",
    "        Returns ∇_a.Q(s, a|θ^µ)\n",
    "        '''\n",
    "        \n",
    "        return np.array(self.sess.run(self.action_gradients,\n",
    "                            feed_dict = {\n",
    "                                self.state: state,\n",
    "                                self.action: action,\n",
    "                                self.sequence_length: sequence_length})[0])\n",
    "\n",
    "    def init_target_network(self):\n",
    "        \n",
    "        self.sess.run(self.init_target_network_params)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \n",
    "        self.sess.run(self.update_target_network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \n",
    "    '''\n",
    "    Replay memory D in article\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, state, action, reward, n_state):\n",
    "        self.buffer.append([state, action, reward, n_state])\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay(replay_memory, batch_size, actor, critic, embeddings, ra_length, state_space_size, action_space_size, discount_factor):\n",
    "    \n",
    "    '''\n",
    "    Experience replay.\n",
    "    Args:\n",
    "          replay_memory: replay memory D in article.\n",
    "          batch_size: sample size.\n",
    "          actor: Actor network.\n",
    "          critic: Critic network.\n",
    "          embeddings: Embeddings object.\n",
    "          state_space_size: dimension of states.\n",
    "          action_space_size: dimensions of actions.\n",
    "    Returns:\n",
    "          Best Q-value, loss of Critic network for printing/recording purpose.\n",
    "    '''\n",
    "\n",
    "    # Sample minibatch of N transitions (s, a, r, s′)\n",
    "    samples = replay_memory.sample_batch(batch_size)\n",
    "    states = np.array([s[0] for s in samples])\n",
    "    actions = np.array([s[1] for s in samples])\n",
    "    rewards = np.array([s[2] for s in samples])\n",
    "    n_states = np.array([s[3] for s in samples]).reshape(-1, state_space_size)\n",
    "\n",
    "    # Generate a′ by target Actor network \n",
    "    n_actions = actor.get_recommendation_list(ra_length, states, embeddings, target = True).reshape(-1, action_space_size)\n",
    "\n",
    "    # Calculate predicted Q′(s′, a′|θ^µ′) value\n",
    "    target_Q_value = critic.predict_target(n_states, n_actions, [ra_length] * batch_size)\n",
    "\n",
    "    # Set y = r + γQ′(s′, a′|θ^µ′)'\n",
    "    expected_rewards = rewards + discount_factor * target_Q_value\n",
    "    \n",
    "    # Update Critic by minimizing (y − Q(s, a|θ^µ))²'\n",
    "    critic_Q_value, critic_loss, _ = critic.train(states, actions, [ra_length] * batch_size, expected_rewards)\n",
    "    \n",
    "    # Update the Actor using the sampled policy gradient'\n",
    "    action_gradients = critic.get_action_gradients(states, n_actions, [ra_length] * batch_size)\n",
    "    actor.train(states, [ra_length] * batch_size, action_gradients)\n",
    "\n",
    "    # Update the Critic target networks\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Update the Actor target network'\n",
    "    actor.update_target_network()\n",
    "\n",
    "    return np.amax(critic_Q_value), critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise():\n",
    "    \n",
    "    '''\n",
    "    Noise for Actor predictions\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, action_space_size, mu = 0, theta = 0.5, sigma = 0.2):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_space_size) * self.mu\n",
    "\n",
    "    def get(self):\n",
    "        self.state += self.theta * (self.mu - self.state) + self.sigma * np.random.rand(self.action_space_size)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_train(sess, environment, actor, critic, embeddings, history_length, ra_length, buffer_size, batch_size, discount_factor, nb_episodes, filename_summary, nb_rounds):\n",
    "\n",
    "    # Set up summary operators\n",
    "    def build_summaries():\n",
    "        episode_reward = tf.Variable(0.)\n",
    "        tf.summary.scalar('reward', episode_reward)\n",
    "        episode_max_Q = tf.Variable(0.)\n",
    "        tf.summary.scalar('max_Q_value', episode_max_Q)\n",
    "        critic_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('critic_loss', critic_loss)\n",
    "\n",
    "        summary_vars = [episode_reward, episode_max_Q, critic_loss]\n",
    "        summary_ops = tf.summary.merge_all()\n",
    "        return summary_ops, summary_vars\n",
    "\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(filename_summary, sess.graph)\n",
    "\n",
    "    # Initialize target network f′ and Q′'\n",
    "    actor.init_target_network()\n",
    "    critic.init_target_network()\n",
    "\n",
    "    # Initialize the capacity of replay memory D'\n",
    "    replay_memory = ReplayMemory(buffer_size) # Memory D \n",
    "    replay = False\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i_session in range(nb_episodes): \n",
    "        session_reward = 0\n",
    "        session_Q_value = 0\n",
    "        session_critic_loss = 0\n",
    "\n",
    "        states = environment.reset() # Initialize state s_0 from previous sessions\n",
    "        \n",
    "        if (i_session + 1) % 10 == 0: # Update average parameters every 10 episodes\n",
    "            environment.groups = environment.get_groups()\n",
    "          \n",
    "        exploration_noise = Noise(history_length * embeddings.size())\n",
    "\n",
    "        for t in range(nb_rounds): \n",
    "            # Transition Generating Stage\n",
    "            # Select an action a_t = {a_t^1, ..., a_t^K}\n",
    "            actions = actor.get_recommendation_list(\n",
    "                ra_length,\n",
    "                states.reshape(1, -1) + exploration_noise.get().reshape(1, -1),\n",
    "                embeddings).reshape(ra_length, embeddings.size())\n",
    "\n",
    "            # Execute action a_t and observe the reward list {r_t^1, ..., r_t^K} for each item in a_t'\n",
    "            rewards, next_states = environment.step(actions)\n",
    "\n",
    "            # 'Store transition (s_t, a_t, r_t, s_t+1) \n",
    "            replay_memory.add(states.reshape(history_length * embeddings.size()),\n",
    "                              actions.reshape(ra_length * embeddings.size()),\n",
    "                              [rewards],\n",
    "                              next_states.reshape(history_length * embeddings.size()))\n",
    "\n",
    "            states = next_states # Set s_t = s_t+1'\n",
    "\n",
    "            session_reward += rewards\n",
    "            \n",
    "            # Parameter Updating Stage\n",
    "            if replay_memory.size() >= batch_size: # Experience replay\n",
    "                replay = True\n",
    "                replay_Q_value, critic_loss = experience_replay(replay_memory, batch_size,\n",
    "                  actor, critic, embeddings, ra_length, history_length * embeddings.size(),\n",
    "                  ra_length * embeddings.size(), discount_factor)\n",
    "                session_Q_value += replay_Q_value\n",
    "                session_critic_loss += critic_loss\n",
    "\n",
    "            summary_str = sess.run(summary_ops,\n",
    "                                  feed_dict = {summary_vars[0]: session_reward,\n",
    "                                              summary_vars[1]: session_Q_value,\n",
    "                                              summary_vars[2]: session_critic_loss})\n",
    "            \n",
    "            writer.add_summary(summary_str, i_session)\n",
    "\n",
    "        str_loss = str('Loss = %0.4f' % session_critic_loss)\n",
    "        print(('Episode %d/%d Time = %ds ' + (str_loss if replay else 'No replay')) % (i_session + 1, nb_episodes, time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "history_length = 10 # N in session\n",
    "ra_length = 3 # K in session\n",
    "discount_factor = 0.99 # Gamma in Bellman equation\n",
    "actor_lr = 0.00005\n",
    "critic_lr = 0.001\n",
    "tau = 0.001 \n",
    "batch_size = 64\n",
    "nb_rounds = 50\n",
    "nb_episodes = 5\n",
    "filename_summary = 'summary.txt'\n",
    "alpha = 0.2 \n",
    "gamma = 0.9 \n",
    "buffer_size = 10000 # Size of replay memory D \n",
    "fixed_length = True # Fixed memory length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGenerator('../data/user_mini_data.tar.gz', '../data/music_mini_data.tar.gz')\n",
    "dg.gen_train_test(train_ratio = 0.7, seed = 42)\n",
    "dg.write_csv('train_set.csv', dg.train, nb_states = [history_length], nb_actions = [ra_length])\n",
    "dg.write_csv('test_set.csv', dg.test, nb_states = [history_length], nb_actions = [ra_length])\n",
    "data = read_file('train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Generate embeddings\n",
    "    eg = EmbeddingsGenerator(dg.user_train, dg.data)\n",
    "    eg.train(nb_epochs = 200)\n",
    "    eg.save_embeddings('../src/embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings(read_embeddings('../src/embeddings.csv'))\n",
    "\n",
    "state_space_size = embeddings.size() * history_length\n",
    "action_space_size = embeddings.size() * ra_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment(data, embeddings, alpha, gamma, fixed_length)\n",
    "tf.reset_default_graph() # For multiple consecutive executions\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initialize actor network f_θ^π and critic network Q(s, a|θ^µ) with random weights\n",
    "actor = Actor(sess, state_space_size, action_space_size, batch_size, ra_length, history_length, embeddings.size(), tau, actor_lr)\n",
    "critic = Critic(sess, state_space_size, action_space_size, history_length, embeddings.size(), tau, critic_lr)\n",
    "agent_train(sess, environment, actor, critic, embeddings, history_length, ra_length, buffer_size, batch_size, discount_factor, nb_episodes, filename_summary, nb_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_embeddings = {}\n",
    "for i, item in enumerate(embeddings.get_embedding_vector()):\n",
    "    str_item = str(item)\n",
    "    assert(str_item not in dict_embeddings)\n",
    "    dict_embeddings[str_item] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_items(state, actor, ra_length, embeddings, dict_embeddings, target = False):\n",
    "    return [dict_embeddings[str(action)]\n",
    "          for action in actor.get_recommendation_list(ra_length, np.array(state).reshape(1, -1), \n",
    "                                                      embeddings, target).reshape(ra_length, embeddings.size())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_columns = ['acousticness','beat_strength', 'bounciness', 'danceability', \n",
    "                 'energy', 'liveness', 'speechiness', 'valence']\n",
    "def test_actor(actor, test_df, all_df, embeddings, dict_embeddings, ra_length, history_length, target = False, nb_rounds = 2):\n",
    "    ratings = []\n",
    "    unknown = 0\n",
    "    diversity = []\n",
    "\n",
    "    for _ in range(nb_rounds):\n",
    "        for i in range(len(test_df)):\n",
    "            history_sample = list(test_df[i].sample(history_length)['music_id'])\n",
    "            recommendation = state_to_items(embeddings.embed(history_sample), actor, ra_length, embeddings, dict_embeddings, target)\n",
    "            features = []\n",
    "            for item in recommendation:\n",
    "                l = list(test_df[i].loc[test_df[i]['music_id']==item]['rating'])\n",
    "                if len(l) == 0:\n",
    "                    unknown += 1\n",
    "                else:\n",
    "                    ratings.append(l[0])\n",
    "\n",
    "                features.append(all_df.loc[all_df['music_id']==item][music_columns].iloc[0].tolist())\n",
    "\n",
    "            similarity = cosine_similarity(features)\n",
    "            upper_right = np.triu_indices(similarity.shape[0], k = 1)\n",
    "            diversity.append(1-np.mean(similarity[upper_right]))\n",
    "\n",
    "    return ratings, diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "ratings, diversity = test_actor(actor, dg.test, dg.data, embeddings, dict_embeddings, ra_length, history_length, target = False, nb_rounds = 2)\n",
    "ratings = pd.DataFrame(ratings)\n",
    "\n",
    "test_rating, baseline_rating = (1-ratings.mean())*100,(1-dg.data.rating.mean())*100\n",
    "all_ratings = [test_rating, baseline_rating]\n",
    "fig, ax = plt.subplots(figsize = (7,4))\n",
    "labels = ['RL', 'Traditional methods']\n",
    "x = np.arange(len(labels))\n",
    "results = ax.bar(x, all_ratings, width = 0.5)\n",
    "ax.set_ylabel('Rate%', fontsize = 15)\n",
    "ax.set_title('Skip rate', fontsize = 15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
