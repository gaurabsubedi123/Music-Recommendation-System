{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import keras.backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    \n",
    "    def __init__(self, datapath, itempath):\n",
    "        \n",
    "        '''\n",
    "        Load data \n",
    "        List the users and the music items\n",
    "        List all the users historic\n",
    "        '''\n",
    "        \n",
    "        self.data  = self.load_datas(datapath, itempath)\n",
    "        self.users = self.data['session_id'].unique()  #list of all users\n",
    "        self.histo = self.gen_histo()\n",
    "        self.train = []\n",
    "        self.test  = []\n",
    "\n",
    "    def load_music_data(self, itempath):\n",
    "        \n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        datapath :  string\n",
    "                    path to the music data\n",
    "        '''\n",
    "        \n",
    "        music = pd.read_csv(itempath, compression = 'gzip')\n",
    "        music = music.rename(columns = {music.columns[0]:'track_id'})\n",
    "        music = music[music.us_popularity_estimate>=95] # Focus on the song tracks that have good popularity\n",
    "\n",
    "        return music\n",
    "    \n",
    "    def load_user_data(self, datapath):\n",
    "        \n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        datapath :  string\n",
    "                    path to the session data\n",
    "        '''\n",
    "        \n",
    "        user = pd.read_csv(datapath, compression = 'gzip', nrows = 100000)\n",
    "        user = user.rename(columns = {user.columns[0]:'session_id'})\n",
    "        \n",
    "        # Correct the labeling of skip\n",
    "        # rating = 1 if user doesn't the skip the music, otherwise rating = 0\n",
    "        user['rating'] = user.skip_2.astype(int)*-1+1\n",
    "        \n",
    "        return user\n",
    "    \n",
    "    def load_datas(self, datapath, itempath):\n",
    "        \n",
    "        '''\n",
    "        Load the data and merge the id of each song track. \n",
    "        A row corresponds to a rate given by a user to a song.\n",
    "        '''\n",
    "        \n",
    "        music = self.load_music_data(itempath)\n",
    "        data = self.load_user_data(datapath)\n",
    "        data = pd.merge(data, music, how = 'inner', left_on = ['track_id_clean'], right_on = ['track_id'])\n",
    "        data = data.groupby(['session_id','session_length']).filter(lambda x: len(x) == 20) # Filter out the sessions that are too short\n",
    "        data['session_id'] = LabelEncoder().fit_transform(data['session_id'])\n",
    "        data['music_id'] = LabelEncoder().fit_transform(data['track_id'])\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def gen_histo(self):\n",
    "        \n",
    "        '''\n",
    "        Group all rates given by users and store them from older to most recent.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        result :    List(DataFrame)\n",
    "                    List of the historic for each user\n",
    "        '''\n",
    "        \n",
    "        historic_users = []\n",
    "        for i, u in enumerate(self.users):\n",
    "            temp = self.data[self.data['session_id'] == u]\n",
    "            temp = temp.sort_values('session_position').reset_index()\n",
    "            temp.drop('index', axis = 1, inplace = True)\n",
    "            historic_users.append(temp)\n",
    "        return historic_users\n",
    "    \n",
    "    def sample_histo(self, user_histo, action_ratio = 0.8, max_samp_by_user = 10,  max_state = 10, max_action = 5,\n",
    "                 nb_states = [], nb_actions = []):\n",
    "        \n",
    "        '''\n",
    "        For a given historic, make one or multiple sampling.\n",
    "        If no optional argument given for nb_states and nb_actions, then the sampling\n",
    "        is random and each sample can have differents size for action and state.\n",
    "        To normalize sampling we need to give list of the numbers of states and actions\n",
    "        to be sampled.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_histo :  DataFrame\n",
    "                          historic of user\n",
    "        delimiter :       string, optional\n",
    "                          delimiter for the csv\n",
    "        action_ratio :    float, optional\n",
    "                          ratio form which song tracks in history will be selected\n",
    "        max_samp_by_user: int, optional\n",
    "                          Nulber max of sample to make by user\n",
    "        max_state :       int, optional\n",
    "                          Number max of song tracks to take for the 'state' column\n",
    "        max_action :      int, optional\n",
    "                          Number max of song tracks to take for the 'action' action\n",
    "        nb_states :       array(int), optional\n",
    "                          Numbers of song tracks to be taken for each sample made on user's historic\n",
    "        nb_actions :      array(int), optional\n",
    "                          Numbers of rating to be taken for each sample made on user's historic\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        states :         List(String)\n",
    "                        All the states sampled, format of a sample: itemId&rating\n",
    "        actions :        List(String)\n",
    "                        All the actions sampled, format of a sample: itemId&rating\n",
    "      \n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        States must be before(timestamp<) the actions.\n",
    "        If given, size of nb_states is the numbller of sample by user\n",
    "        sizes of nb_states and nb_actions must be equals\n",
    "        '''\n",
    "\n",
    "        n = len(user_histo)\n",
    "        sep = int(action_ratio * n)\n",
    "        nb_sample = random.randint(1, max_samp_by_user)\n",
    "        if not nb_states:\n",
    "            nb_states = [min(random.randint(1, sep), max_state) for i in range(nb_sample)]\n",
    "        if not nb_actions:\n",
    "            nb_actions = [min(random.randint(1, n - sep), max_action) for i in range(nb_sample)]\n",
    "        assert len(nb_states) == len(nb_actions), 'Given array must have the same size'\n",
    "        \n",
    "        states  = []\n",
    "        actions = []\n",
    "        # Select samples in histo \n",
    "        for i in range(len(nb_states)):\n",
    "            sample_states = user_histo.iloc[0:sep].sample(nb_states[i])\n",
    "            sample_actions = user_histo.iloc[-(n - sep):].sample(nb_actions[i])\n",
    "            \n",
    "            sample_state  = []\n",
    "            sample_action = []\n",
    "            for j in range(nb_states[i]):\n",
    "                row = sample_states.iloc[j]\n",
    "                # Formate State\n",
    "                state = str(row.loc['music_id'])+'&'+str(row.loc['rating'].astype(int)) #TODO: REWARD SHAPING\n",
    "                sample_state.append(state)\n",
    "          \n",
    "            for j in range(nb_actions[i]):\n",
    "                row = sample_actions.iloc[j]\n",
    "                # Formate Action \n",
    "                action = str(row.loc['music_id'])+'&'+str(row.loc['rating'].astype(int))\n",
    "                sample_action.append(action)\n",
    "\n",
    "            states.append(sample_state)\n",
    "            actions.append(sample_action)\n",
    "                \n",
    "        return states, actions\n",
    "    \n",
    "    def gen_train_test(self, train_ratio, seed = None):\n",
    "        \n",
    "        '''\n",
    "        Shuffle the historic of users and separate it in a train and a test set.\n",
    "        Store the ids for each set.\n",
    "        An user can't be in both set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        test_ratio :  float\n",
    "                      Ratio to control the sizes of the sets\n",
    "        seed       :  float\n",
    "                      Seed on the shuffle\n",
    "        '''\n",
    "        \n",
    "        n = len(self.histo)\n",
    "\n",
    "        if seed is not None:\n",
    "            random.Random(seed).shuffle(self.histo)\n",
    "        else:\n",
    "            random.shuffle(self.histo)\n",
    "\n",
    "        self.train = self.histo[:int((train_ratio * n))]\n",
    "        self.test  = self.histo[int((train_ratio * n)):]\n",
    "        self.user_train = [h.iloc[0,0] for h in self.train]\n",
    "        self.user_test  = [h.iloc[0,0] for h in self.test]\n",
    "        \n",
    "    def write_csv(self, filename, histo_to_write, delimiter = ';', action_ratio = 0.8, max_samp_by_user = 10,\n",
    "                  max_state = 10, max_action = 5, nb_states = [], nb_actions = []):\n",
    "        \n",
    "        '''\n",
    "        From  a given historic, create a csv file with the format:\n",
    "        columns : state;action_reward;n_state\n",
    "        rows    : itemid&rating1 | itemid&rating2 | ... ; itemid&rating3 | ...  | ... | item&rating4\n",
    "        at filename location.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename :        string\n",
    "                          path to the file to be produced\n",
    "        histo_to_write :  List(DataFrame)\n",
    "                          List of the historic for each user\n",
    "        delimiter :       string, optional\n",
    "                          delimiter for the csv\n",
    "        action_ratio :    float, optional\n",
    "                          ratio form which song tracks in history will be selected\n",
    "        max_samp_by_user: int, optional\n",
    "                          Nulber max of sample to make by user\n",
    "        max_state :       int, optional\n",
    "                          Number max of song tracks to take for the 'state' column\n",
    "        max_action :      int, optional\n",
    "                          Number max of song tracks to take for the 'action' action\n",
    "        nb_states :       array(int), optional\n",
    "                          Numbers of song tracks to be taken for each sample made on user's historic\n",
    "        nb_actions :      array(int), optional\n",
    "                          Numbers of rating to be taken for each sample made on user's historic\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        if given, size of nb_states is the numbller of sample by user\n",
    "        sizes of nb_states and nb_actions must be equals\n",
    "\n",
    "        '''\n",
    "        \n",
    "        with open(filename, mode = 'w') as file:\n",
    "            f_writer = csv.writer(file, delimiter = delimiter)\n",
    "            f_writer.writerow(['state', 'action_reward', 'n_state'])\n",
    "            for user_histo in histo_to_write:\n",
    "                states, actions = self.sample_histo(user_histo, action_ratio, max_samp_by_user, max_state, max_action, nb_states, nb_actions)\n",
    "                for i in range(len(states)):\n",
    "                    # FORMAT STATE\n",
    "                    state_str   = '|'.join(states[i])\n",
    "                    # FORMAT ACTION\n",
    "                    action_str  = '|'.join(actions[i])\n",
    "                    # FORMAT N_STATE\n",
    "                    n_state_str = state_str + '|' + action_str\n",
    "                    f_writer.writerow([state_str, action_str, n_state_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings():\n",
    "\n",
    "    '''\n",
    "    Returns Embeddings and embedding metadata after initialization with item_embeddings\n",
    "    '''\n",
    "\n",
    "    def __init__(self, item_embeddings):\n",
    "        self.item_embeddings = item_embeddings\n",
    "    \n",
    "    def size(self):\n",
    "        return self.item_embeddings.shape[1]\n",
    "    \n",
    "    def get_embedding_vector(self):\n",
    "        return self.item_embeddings\n",
    "    \n",
    "    def get_embedding(self, item_index):\n",
    "        return self.item_embeddings[item_index]\n",
    "\n",
    "    def embed(self, item_list):\n",
    "        return np.array([self.get_embedding(item) for item in item_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsGenerator():\n",
    "\n",
    "    '''\n",
    "    Returns Embeddings after initialization with train_users and data\n",
    "    '''\n",
    "    \n",
    "    def  __init__(self, train_users, data):\n",
    "        \n",
    "        self.train_users = train_users\n",
    "        self.data = data.sort_values(by=['date'])\n",
    "        self.session_count = self.data['session_id'].max()+1\n",
    "        self.track_count = self.data['music_id'].max()+1\n",
    "        self.session_tracks = {} # list of rated song tracks by each session\n",
    "        for sessionId in range(self.session_count):\n",
    "            self.session_tracks[sessionId] = self.data[self.data.session_id == sessionId]['music_id'].tolist()\n",
    "        self.m = self.model()\n",
    "\n",
    "    def model(self, hidden_layer_size = 300):\n",
    "        \n",
    "        m = Sequential()\n",
    "        m.add(Dense(hidden_layer_size, input_shape = (1, self.track_count)))\n",
    "        m.add(Dense(self.track_count, activation = 'softmax'))\n",
    "        m.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        return m\n",
    "    \n",
    "    def generate_input(self, session_id):\n",
    "        \n",
    "        '''\n",
    "        Returns a context and a target for the user_id\n",
    "        context: user's history with one random song removed\n",
    "        target: id of random removed song\n",
    "        '''\n",
    "        \n",
    "        session_tracks_count = len(self.session_tracks[session_id])\n",
    "        # picking random song\n",
    "        random_index = np.random.randint(0, session_tracks_count-1) # -1 avoids taking the last track\n",
    "        # setting target\n",
    "        target = np.zeros((1, self.track_count))\n",
    "        target[0][self.session_tracks[session_id][random_index]] = 1\n",
    "        # setting context\n",
    "        context = np.zeros((1, self.track_count))\n",
    "        context[0][self.session_tracks[session_id][:random_index] + self.session_tracks[session_id][random_index+1:]] = 1\n",
    "        return context, target\n",
    "\n",
    "    def train(self, nb_epochs, batch_size = 2000):\n",
    "        \n",
    "        '''\n",
    "        Trains the model from train_users's history\n",
    "        '''\n",
    "        \n",
    "        for i in range(nb_epochs):\n",
    "            print('%d/%d' % (i+1, nb_epochs))\n",
    "            batch = [self.generate_input(session_id = np.random.choice(self.train_users)) for _ in range(batch_size)]\n",
    "            X_train = np.array([b[0] for b in batch])\n",
    "            y_train = np.array([b[1] for b in batch])\n",
    "            self.m.fit(X_train, y_train, epochs = 1, validation_split = 0.3)\n",
    "\n",
    "    def test(self, test_users, batch_size = 2000):\n",
    "        \n",
    "        '''\n",
    "        Returns [loss, accuracy] on the test set\n",
    "        '''\n",
    "        \n",
    "        print('test users', len(test_users))\n",
    "        batch_test = [self.generate_input(session_id = np.random.choice(test_users)) for _ in range(batch_size)]\n",
    "        X_test = np.array([b[0] for b in batch_test])\n",
    "        y_test = np.array([b[1] for b in batch_test])\n",
    "        return self.m.evaluate(X_test, y_test)\n",
    "\n",
    "    def save_embeddings(self, file_name):\n",
    "        \n",
    "        '''\n",
    "        Generates a csv file containg the vector embedding for each song\n",
    "        '''\n",
    "        \n",
    "        inp = self.m.input                                           # input placeholder\n",
    "        outputs = [layer.output for layer in self.m.layers]          # all layer outputs\n",
    "        functor = K.function([inp, K.learning_phase()], outputs )    # evaluation function\n",
    "\n",
    "        # append embeddings to vectors\n",
    "        vectors = []\n",
    "        for music_id in range(self.track_count):\n",
    "            track = np.zeros((1, 1, self.track_count))\n",
    "            track[0][0][music_id] = 1\n",
    "            layer_outs = functor([track])\n",
    "            vector = [str(v) for v in layer_outs[0][0][0]]\n",
    "            vector = '|'.join(vector)\n",
    "            vectors.append([music_id, vector])\n",
    "\n",
    "        #saves as a csv file\n",
    "        embeddings = pd.DataFrame(vectors, columns = ['music_id', 'vectors']).astype({'music_id': 'int32'})\n",
    "        embeddings.to_csv(file_name, sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data_path):\n",
    "    \n",
    "    '''\n",
    "    Load data from train_set.csv or test_set.csv\n",
    "    '''\n",
    "\n",
    "    data = pd.read_csv(data_path, sep = ';')\n",
    "    for col in ['state', 'n_state', 'action_reward']:\n",
    "        data[col] = [np.array([[np.int(float(k)) for k in ee.split('&')] for ee in e.split('|')]) for e in data[col]]\n",
    "    for col in ['state', 'n_state']:\n",
    "        data[col] = [np.array([e[0] for e in l]) for l in data[col]]\n",
    "\n",
    "    data['action'] = [[e[0] for e in l] for l in data['action_reward']]\n",
    "    data['reward'] = [tuple(e[1] for e in l) for l in data['action_reward']]\n",
    "    data.drop(columns = ['action_reward'], inplace = True)\n",
    "\n",
    "    return data\n",
    "\n",
    "def read_embeddings(embeddings_path):\n",
    "    \n",
    "    '''\n",
    "    Load embeddings (a vector for each item)\n",
    "    '''\n",
    "    \n",
    "    embeddings = pd.read_csv(embeddings_path, sep = ';')\n",
    "\n",
    "    return np.array([[np.float64(k) for k in e.split('|')] for e in embeddings['vectors']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self, data, embeddings, alpha, gamma, fixed_length):\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        self.embedded_data = pd.DataFrame()\n",
    "        self.embedded_data['state'] = [np.array([embeddings.get_embedding(item_id) \n",
    "                                                 for item_id in row['state']]) for _, row in data.iterrows()]\n",
    "        self.embedded_data['action'] = [np.array([embeddings.get_embedding(item_id) \n",
    "                                                  for item_id in row['action']]) for _, row in data.iterrows()]\n",
    "        self.embedded_data['reward'] = data['reward']\n",
    "\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "        self.fixed_length = fixed_length\n",
    "        self.current_state = self.reset()\n",
    "        self.groups = self.get_groups()\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.init_state = self.embedded_data['state'].sample(1).values[0]\n",
    "        return self.init_state\n",
    "\n",
    "    def step(self, actions):\n",
    "        \n",
    "        '''\n",
    "        Compute reward and update state.\n",
    "        Args:\n",
    "          actions: embedded chosen items.\n",
    "        Returns:\n",
    "          cumulated_reward: overall reward.\n",
    "          current_state: updated state.\n",
    "        '''\n",
    "\n",
    "        # Compute overall reward \n",
    "        simulated_rewards, cumulated_reward = self.simulate_rewards(self.current_state.reshape((1, -1)), actions.reshape((1, -1)))\n",
    "\n",
    "        for k in range(len(simulated_rewards)): \n",
    "            if simulated_rewards[k] > 0:\n",
    "                self.current_state = np.append(self.current_state, [actions[k]], axis = 0)\n",
    "                if self.fixed_length: \n",
    "                    self.current_state = np.delete(self.current_state, 0, axis = 0)\n",
    "\n",
    "        return cumulated_reward, self.current_state\n",
    "\n",
    "    def get_groups(self):\n",
    "        \n",
    "        '''\n",
    "        Calculate average state/action value for each group\n",
    "        '''\n",
    "\n",
    "        groups = []\n",
    "        for rewards, group in self.embedded_data.groupby(['reward']):\n",
    "            size = group.shape[0]\n",
    "            states = np.array(list(group['state'].values))\n",
    "            actions = np.array(list(group['action'].values))\n",
    "            groups.append({\n",
    "                'size': size,\n",
    "                'rewards': rewards, \n",
    "                'average state': (np.sum(states / np.linalg.norm(states, 2, axis = 1)[:, np.newaxis], axis = 0) / size).reshape((1, -1)), # s_x^-\n",
    "                'average action': (np.sum(actions / np.linalg.norm(actions, 2, axis = 1)[:, np.newaxis], axis = 0) / size).reshape((1, -1)) # a_x^-\n",
    "              })\n",
    "        return groups\n",
    "\n",
    "    def simulate_rewards(self, current_state, chosen_actions, reward_type = 'grouped cosine'):\n",
    "        \n",
    "        '''\n",
    "        Calculate simulated rewards.\n",
    "        Args:\n",
    "          current_state: history, list of embedded items.\n",
    "          chosen_actions: embedded chosen items.\n",
    "          reward_type: from ['normal', 'grouped average', 'grouped cosine'].\n",
    "        Returns:\n",
    "          returned_rewards: most probable rewards.\n",
    "          cumulated_reward: probability weighted rewards.\n",
    "        '''\n",
    "\n",
    "        def cosine_state_action(s_t, a_t, s_i, a_i):\n",
    "            cosine_state = np.dot(s_t, s_i.T) / (np.linalg.norm(s_t, 2) * np.linalg.norm(s_i, 2))\n",
    "            cosine_action = np.dot(a_t, a_i.T) / (np.linalg.norm(a_t, 2) * np.linalg.norm(a_i, 2))\n",
    "            return (self.alpha * cosine_state + (1 - self.alpha) * cosine_action).reshape((1,))\n",
    "\n",
    "        if reward_type == 'normal':\n",
    "            probabilities = [cosine_state_action(current_state, chosen_actions, row['state'], row['action'])\n",
    "                           for _, row in self.embedded_data.iterrows()]\n",
    "        elif reward_type == 'grouped average':\n",
    "            probabilities = np.array([g['size'] for g in self.groups]) *\\\n",
    "            [(self.alpha * (np.dot(current_state, g['average state'].T) / np.linalg.norm(current_state, 2))\\\n",
    "            + (1 - self.alpha) * (np.dot(chosen_actions, g['average action'].T) / np.linalg.norm(chosen_actions, 2)))\n",
    "             for g in self.groups]\n",
    "        elif reward_type == 'grouped cosine':\n",
    "            probabilities = [cosine_state_action(current_state, chosen_actions, g['average state'], g['average action']) \n",
    "                           for g in self.groups]\n",
    "\n",
    "        # Normalize \n",
    "        probabilities = np.array(probabilities) / sum(probabilities)\n",
    "\n",
    "        # Get most probable rewards\n",
    "        if reward_type == 'normal':\n",
    "            returned_rewards = self.embedded_data.iloc[np.argmax(probabilities)]['reward']\n",
    "        elif reward_type in ['grouped average', 'grouped cosine']:\n",
    "            returned_rewards = self.groups[np.argmax(probabilities)]['rewards']\n",
    "\n",
    "        def overall_reward(rewards, gamma):\n",
    "            return np.sum([gamma**k * reward for k, reward in enumerate(rewards)])\n",
    "\n",
    "        if reward_type in ['normal', 'grouped average']:\n",
    "            cumulated_reward = overall_reward(returned_rewards, self.gamma)\n",
    "        elif reward_type == 'grouped cosine':\n",
    "            # Get probability weighted cumulated reward\n",
    "            cumulated_reward = np.sum([p * overall_reward(g['rewards'], self.gamma) for p, g in zip(probabilities, self.groups)])\n",
    "\n",
    "        return returned_rewards, cumulated_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    \n",
    "    '''\n",
    "    Policy function approximator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, sess, state_space_size, action_space_size, batch_size, ra_length, history_length, embedding_size, tau, learning_rate, scope = 'actor'):\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.batch_size = batch_size\n",
    "        self.ra_length = ra_length\n",
    "        self.history_length = history_length\n",
    "        self.embedding_size = embedding_size\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope = scope\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # Build Actor network\n",
    "            self.action_weights, self.state, self.sequence_length = self._build_net('estimator_actor')\n",
    "            self.network_params = tf.trainable_variables()\n",
    "\n",
    "            # Build target Actor network\n",
    "            self.target_action_weights, self.target_state, self.target_sequence_length = self._build_net('target_actor')\n",
    "            self.target_network_params = tf.trainable_variables()[len(self.network_params):] # TODO: why sublist [len(x):]? Maybe because its equal to network_params + target_network_params\n",
    "\n",
    "            # Initialize target network weights with network weights\n",
    "            self.init_target_network_params = [self.target_network_params[i].assign(self.network_params[i]) \n",
    "                                               for i in range(len(self.target_network_params))]\n",
    "\n",
    "            # Update target network weights \n",
    "            self.update_target_network_params = [self.target_network_params[i].assign(\n",
    "            tf.multiply(self.tau, self.network_params[i]) +\n",
    "            tf.multiply(1 - self.tau, self.target_network_params[i]))for i in range(len(self.target_network_params))]\n",
    "\n",
    "            # Gradient computation from Critic's action_gradients\n",
    "            self.action_gradients = tf.placeholder(tf.float32, [None, self.action_space_size])\n",
    "            gradients = tf.gradients(tf.reshape(self.action_weights, [self.batch_size, self.action_space_size], name = '42'),\n",
    "                                     self.network_params, self.action_gradients)\n",
    "            params_gradients = list(map(lambda x: tf.div(x, self.batch_size * self.action_space_size), gradients))\n",
    "\n",
    "            # Compute ∇_a.Q(s, a|θ^µ).∇_θ^π.f_θ^π(s)\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(zip(params_gradients, self.network_params))\n",
    "\n",
    "    def _build_net(self, scope):\n",
    "        \n",
    "        '''\n",
    "        Build the (target) Actor network\n",
    "        '''\n",
    "\n",
    "        def gather_last_output(data, seq_lens):\n",
    "            def cli_value(x, v):\n",
    "                y = tf.constant(v, shape = x.get_shape(), dtype = tf.int64)\n",
    "                x = tf.cast(x, tf.int64)\n",
    "                return tf.where(tf.greater(x, y), x, y)\n",
    "\n",
    "            batch_range = tf.range(tf.cast(tf.shape(data)[0], dtype = tf.int64), dtype = tf.int64)\n",
    "            tmp_end = tf.map_fn(lambda x: cli_value(x, 0), seq_lens - 1, dtype = tf.int64)\n",
    "            indices = tf.stack([batch_range, tmp_end], axis = 1)\n",
    "            return tf.gather_nd(data, indices)\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            # Inputs: current state, sequence_length\n",
    "            # Outputs: action weights \n",
    "            state = tf.placeholder(tf.float32, [None, self.state_space_size], 'state')\n",
    "            state_ = tf.reshape(state, [-1, self.history_length, self.embedding_size])\n",
    "            sequence_length = tf.placeholder(tf.int32, [None], 'sequence_length')\n",
    "            cell = tf.nn.rnn_cell.GRUCell(self.embedding_size,\n",
    "                                        activation = tf.nn.relu,\n",
    "                                        kernel_initializer = tf.initializers.random_normal(),\n",
    "                                        bias_initializer = tf.zeros_initializer())\n",
    "            outputs, _ = tf.nn.dynamic_rnn(cell, state_, dtype = tf.float32, sequence_length = sequence_length)\n",
    "            last_output = gather_last_output(outputs, sequence_length) # TODO: replace by h\n",
    "            x = tf.keras.layers.Dense(self.ra_length * self.embedding_size)(last_output)\n",
    "            action_weights = tf.reshape(x, [-1, self.ra_length, self.embedding_size])\n",
    "\n",
    "        return action_weights, state, sequence_length\n",
    "\n",
    "    def train(self, state, sequence_length, action_gradients):\n",
    "        \n",
    "        '''\n",
    "        Compute ∇_a.Q(s, a|θ^µ).∇_θ^π.f_θ^π(s)\n",
    "        '''\n",
    "        \n",
    "        self.sess.run(self.optimizer,\n",
    "                      feed_dict = {\n",
    "                          self.state: state,\n",
    "                          self.sequence_length: sequence_length,\n",
    "                          self.action_gradients: action_gradients})\n",
    "\n",
    "    def predict(self, state, sequence_length):\n",
    "        \n",
    "        return self.sess.run(self.action_weights,\n",
    "                            feed_dict = {\n",
    "                                self.state: state,\n",
    "                                self.sequence_length: sequence_length})\n",
    "\n",
    "    def predict_target(self, state, sequence_length):\n",
    "        \n",
    "        return self.sess.run(self.target_action_weights,\n",
    "                            feed_dict = {\n",
    "                                self.target_state: state,\n",
    "                                self.target_sequence_length: sequence_length})\n",
    "\n",
    "    def init_target_network(self):\n",
    "        \n",
    "        self.sess.run(self.init_target_network_params)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \n",
    "        self.sess.run(self.update_target_network_params)\n",
    "      \n",
    "    def get_recommendation_list(self, ra_length, noisy_state, embeddings, target = False):\n",
    "        \n",
    "        '''\n",
    "        Args:\n",
    "          ra_length: length of the recommendation list.\n",
    "          noisy_state: current/remembered environment state with noise.\n",
    "          embeddings: Embeddings object.\n",
    "          target: boolean to use Actor's network or target network.\n",
    "        Returns:\n",
    "          Recommendation List: list of embedded items as future actions.\n",
    "        '''\n",
    "\n",
    "        def get_score(weights, embedding, batch_size):\n",
    "            \n",
    "            '''\n",
    "            Args:\n",
    "            weights: w_t^k shape = (embedding_size,).\n",
    "            embedding: e_i shape = (embedding_size,).\n",
    "            Returns:\n",
    "            score of the item i: score_i = w_t^k.e_i^T shape = (1,).\n",
    "            '''\n",
    "\n",
    "            ret = np.dot(weights, embedding.T)\n",
    "            return ret\n",
    "\n",
    "        batch_size = noisy_state.shape[0]\n",
    "\n",
    "        # Generate w_t = {w_t^1, ..., w_t^K}\n",
    "        method = self.predict_target if target else self.predict\n",
    "        weights = method(noisy_state, [ra_length] * batch_size)\n",
    "\n",
    "        # Score items\n",
    "        scores = np.array([[[get_score(weights[i][k], embedding, batch_size)\n",
    "                             for embedding in embeddings.get_embedding_vector()]\n",
    "                            for k in range(ra_length)] for i in range(batch_size)])\n",
    "\n",
    "        # return a_t\n",
    "        return np.array([[embeddings.get_embedding(np.argmax(scores[i][k]))\n",
    "                          for k in range(ra_length)] for i in range(batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    \n",
    "    '''\n",
    "    Value function approximator\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, sess, state_space_size, action_space_size, history_length, embedding_size, tau, learning_rate, scope = 'critic'):\n",
    "        self.sess = sess\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.history_length = history_length\n",
    "        self.embedding_size = embedding_size\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scope = scope\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # Build Critic network\n",
    "            self.critic_Q_value, self.state, self.action, self.sequence_length = self._build_net('estimator_critic')\n",
    "            self.network_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'estimator_critic')\n",
    "\n",
    "            # Build target Critic network\n",
    "            self.target_Q_value, self.target_state, self.target_action, self.target_sequence_length = self._build_net('target_critic')\n",
    "            self.target_network_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'target_critic')\n",
    "\n",
    "            # Initialize target network weights with network weights (θ^µ′ ← θ^µ)\n",
    "            self.init_target_network_params = [self.target_network_params[i].assign(self.network_params[i])\n",
    "            for i in range(len(self.target_network_params))]\n",
    "\n",
    "            # Update target network weights (θ^µ′ ← τθ^µ + (1 − τ)θ^µ′)\n",
    "            self.update_target_network_params = [self.target_network_params[i].assign(\n",
    "            tf.multiply(self.tau, self.network_params[i]) +\n",
    "            tf.multiply(1 - self.tau, self.target_network_params[i]))\n",
    "            for i in range(len(self.target_network_params))]\n",
    "\n",
    "            # Minimize MSE between Critic's and target Critic's outputed Q-values\n",
    "            self.expected_reward = tf.placeholder(tf.float32, [None, 1])\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.expected_reward, self.critic_Q_value))\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "            # Compute ∇_a.Q(s, a|θ^µ)\n",
    "            self.action_gradients = tf.gradients(self.critic_Q_value, self.action)\n",
    "\n",
    "    def _build_net(self, scope):\n",
    "\n",
    "        '''\n",
    "        Build the (target) Critic network\n",
    "        '''\n",
    "\n",
    "        def gather_last_output(data, seq_lens):\n",
    "            def cli_value(x, v):\n",
    "                y = tf.constant(v, shape = x.get_shape(), dtype = tf.int64)\n",
    "                return tf.where(tf.greater(x, y), x, y)\n",
    "\n",
    "            this_range = tf.range(tf.cast(tf.shape(seq_lens)[0], dtype = tf.int64), dtype = tf.int64)\n",
    "            tmp_end = tf.map_fn(lambda x: cli_value(x, 0), seq_lens - 1, dtype = tf.int64)\n",
    "            indices = tf.stack([this_range, tmp_end], axis = 1)\n",
    "            return tf.gather_nd(data, indices)\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            # Inputs: current state, current action\n",
    "            # Outputs: predicted Q-value\n",
    "            state = tf.placeholder(tf.float32, [None, self.state_space_size], 'state')\n",
    "            state_ = tf.reshape(state, [-1, self.history_length, self.embedding_size])\n",
    "            action = tf.placeholder(tf.float32, [None, self.action_space_size], 'action')\n",
    "            sequence_length = tf.placeholder(tf.int64, [None], name = 'critic_sequence_length')\n",
    "            cell = tf.nn.rnn_cell.GRUCell(self.history_length,\n",
    "                                        activation = tf.nn.relu,\n",
    "                                        kernel_initializer = tf.initializers.random_normal(),\n",
    "                                        bias_initializer = tf.zeros_initializer())\n",
    "            predicted_state, _ = tf.nn.dynamic_rnn(cell, state_, dtype = tf.float32, sequence_length = sequence_length)\n",
    "            predicted_state = gather_last_output(predicted_state, sequence_length)\n",
    "\n",
    "            inputs = tf.concat([predicted_state, action], axis = -1)\n",
    "            layer1 = tf.layers.Dense(32, activation = tf.nn.relu)(inputs)\n",
    "            layer2 = tf.layers.Dense(16, activation = tf.nn.relu)(layer1)\n",
    "            critic_Q_value = tf.layers.Dense(1)(layer2)\n",
    "            return critic_Q_value, state, action, sequence_length\n",
    "\n",
    "    def train(self, state, action, sequence_length, expected_reward):\n",
    "        \n",
    "        '''\n",
    "        Minimize MSE between expected reward and target Critic's Q-value\n",
    "        '''\n",
    "        \n",
    "        return self.sess.run([self.critic_Q_value, self.loss, self.optimizer],\n",
    "                            feed_dict = {\n",
    "                                self.state: state,\n",
    "                                self.action: action,\n",
    "                                self.sequence_length: sequence_length,\n",
    "                                self.expected_reward: expected_reward})\n",
    "\n",
    "    def predict(self, state, action, sequence_length):\n",
    "        \n",
    "        '''\n",
    "        Returns Critic's predicted Q-value\n",
    "        '''\n",
    "        \n",
    "        return self.sess.run(self.critic_Q_value,\n",
    "                            feed_dict = {\n",
    "                                self.state: state,\n",
    "                                self.action: action,\n",
    "                                self.sequence_length: sequence_length})\n",
    "\n",
    "    def predict_target(self, state, action, sequence_length):\n",
    "        \n",
    "        '''\n",
    "        Returns target Critic's predicted Q-value\n",
    "        '''\n",
    "        \n",
    "        return self.sess.run(self.target_Q_value,\n",
    "                            feed_dict = {\n",
    "                                self.target_state: state,\n",
    "                                self.target_action: action,\n",
    "                                self.target_sequence_length: sequence_length})\n",
    "\n",
    "    def get_action_gradients(self, state, action, sequence_length):\n",
    "        \n",
    "        '''\n",
    "        Returns ∇_a.Q(s, a|θ^µ)\n",
    "        '''\n",
    "        \n",
    "        return np.array(self.sess.run(self.action_gradients,\n",
    "                            feed_dict = {\n",
    "                                self.state: state,\n",
    "                                self.action: action,\n",
    "                                self.sequence_length: sequence_length})[0])\n",
    "\n",
    "    def init_target_network(self):\n",
    "        \n",
    "        self.sess.run(self.init_target_network_params)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \n",
    "        self.sess.run(self.update_target_network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \n",
    "    '''\n",
    "    Replay memory D in article\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, state, action, reward, n_state):\n",
    "        self.buffer.append([state, action, reward, n_state])\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            self.buffer.pop(0)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay(replay_memory, batch_size, actor, critic, embeddings, ra_length, state_space_size, action_space_size, discount_factor):\n",
    "    \n",
    "    '''\n",
    "    Experience replay.\n",
    "    Args:\n",
    "          replay_memory: replay memory D in article.\n",
    "          batch_size: sample size.\n",
    "          actor: Actor network.\n",
    "          critic: Critic network.\n",
    "          embeddings: Embeddings object.\n",
    "          state_space_size: dimension of states.\n",
    "          action_space_size: dimensions of actions.\n",
    "    Returns:\n",
    "          Best Q-value, loss of Critic network for printing/recording purpose.\n",
    "    '''\n",
    "\n",
    "    # Sample minibatch of N transitions (s, a, r, s′)\n",
    "    samples = replay_memory.sample_batch(batch_size)\n",
    "    states = np.array([s[0] for s in samples])\n",
    "    actions = np.array([s[1] for s in samples])\n",
    "    rewards = np.array([s[2] for s in samples])\n",
    "    n_states = np.array([s[3] for s in samples]).reshape(-1, state_space_size)\n",
    "\n",
    "    # Generate a′ by target Actor network \n",
    "    n_actions = actor.get_recommendation_list(ra_length, states, embeddings, target = True).reshape(-1, action_space_size)\n",
    "\n",
    "    # Calculate predicted Q′(s′, a′|θ^µ′) value\n",
    "    target_Q_value = critic.predict_target(n_states, n_actions, [ra_length] * batch_size)\n",
    "\n",
    "    # Set y = r + γQ′(s′, a′|θ^µ′)'\n",
    "    expected_rewards = rewards + discount_factor * target_Q_value\n",
    "    \n",
    "    # Update Critic by minimizing (y − Q(s, a|θ^µ))²'\n",
    "    critic_Q_value, critic_loss, _ = critic.train(states, actions, [ra_length] * batch_size, expected_rewards)\n",
    "    \n",
    "    # Update the Actor using the sampled policy gradient'\n",
    "    action_gradients = critic.get_action_gradients(states, n_actions, [ra_length] * batch_size)\n",
    "    actor.train(states, [ra_length] * batch_size, action_gradients)\n",
    "\n",
    "    # Update the Critic target networks\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Update the Actor target network'\n",
    "    actor.update_target_network()\n",
    "\n",
    "    return np.amax(critic_Q_value), critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise():\n",
    "    \n",
    "    '''\n",
    "    Noise for Actor predictions\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, action_space_size, mu = 0, theta = 0.5, sigma = 0.2):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_space_size) * self.mu\n",
    "\n",
    "    def get(self):\n",
    "        self.state += self.theta * (self.mu - self.state) + self.sigma * np.random.rand(self.action_space_size)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_train(sess, environment, actor, critic, embeddings, history_length, ra_length, buffer_size, batch_size, discount_factor, nb_episodes, filename_summary, nb_rounds):\n",
    "\n",
    "    # Set up summary operators\n",
    "    def build_summaries():\n",
    "        episode_reward = tf.Variable(0.)\n",
    "        tf.summary.scalar('reward', episode_reward)\n",
    "        episode_max_Q = tf.Variable(0.)\n",
    "        tf.summary.scalar('max_Q_value', episode_max_Q)\n",
    "        critic_loss = tf.Variable(0.)\n",
    "        tf.summary.scalar('critic_loss', critic_loss)\n",
    "\n",
    "        summary_vars = [episode_reward, episode_max_Q, critic_loss]\n",
    "        summary_ops = tf.summary.merge_all()\n",
    "        return summary_ops, summary_vars\n",
    "\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(filename_summary, sess.graph)\n",
    "\n",
    "    # Initialize target network f′ and Q′'\n",
    "    actor.init_target_network()\n",
    "    critic.init_target_network()\n",
    "\n",
    "    # Initialize the capacity of replay memory D'\n",
    "    replay_memory = ReplayMemory(buffer_size) # Memory D \n",
    "    replay = False\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i_session in range(nb_episodes): \n",
    "        session_reward = 0\n",
    "        session_Q_value = 0\n",
    "        session_critic_loss = 0\n",
    "\n",
    "        states = environment.reset() # Initialize state s_0 from previous sessions\n",
    "        \n",
    "        if (i_session + 1) % 10 == 0: # Update average parameters every 10 episodes\n",
    "            environment.groups = environment.get_groups()\n",
    "          \n",
    "        exploration_noise = Noise(history_length * embeddings.size())\n",
    "\n",
    "        for t in range(nb_rounds): \n",
    "            # Transition Generating Stage\n",
    "            # Select an action a_t = {a_t^1, ..., a_t^K}\n",
    "            actions = actor.get_recommendation_list(\n",
    "                ra_length,\n",
    "                states.reshape(1, -1) + exploration_noise.get().reshape(1, -1),\n",
    "                embeddings).reshape(ra_length, embeddings.size())\n",
    "\n",
    "            # Execute action a_t and observe the reward list {r_t^1, ..., r_t^K} for each item in a_t'\n",
    "            rewards, next_states = environment.step(actions)\n",
    "\n",
    "            # 'Store transition (s_t, a_t, r_t, s_t+1) \n",
    "            replay_memory.add(states.reshape(history_length * embeddings.size()),\n",
    "                              actions.reshape(ra_length * embeddings.size()),\n",
    "                              [rewards],\n",
    "                              next_states.reshape(history_length * embeddings.size()))\n",
    "\n",
    "            states = next_states # Set s_t = s_t+1'\n",
    "\n",
    "            session_reward += rewards\n",
    "            \n",
    "            # Parameter Updating Stage\n",
    "            if replay_memory.size() >= batch_size: # Experience replay\n",
    "                replay = True\n",
    "                replay_Q_value, critic_loss = experience_replay(replay_memory, batch_size,\n",
    "                  actor, critic, embeddings, ra_length, history_length * embeddings.size(),\n",
    "                  ra_length * embeddings.size(), discount_factor)\n",
    "                session_Q_value += replay_Q_value\n",
    "                session_critic_loss += critic_loss\n",
    "\n",
    "            summary_str = sess.run(summary_ops,\n",
    "                                  feed_dict = {summary_vars[0]: session_reward,\n",
    "                                              summary_vars[1]: session_Q_value,\n",
    "                                              summary_vars[2]: session_critic_loss})\n",
    "            \n",
    "            writer.add_summary(summary_str, i_session)\n",
    "\n",
    "        str_loss = str('Loss = %0.4f' % session_critic_loss)\n",
    "        print(('Episode %d/%d Time = %ds ' + (str_loss if replay else 'No replay')) % (i_session + 1, nb_episodes, time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "history_length = 10 # N in session\n",
    "ra_length = 3 # K in session\n",
    "discount_factor = 0.99 # Gamma in Bellman equation\n",
    "actor_lr = 0.00005\n",
    "critic_lr = 0.001\n",
    "tau = 0.001 \n",
    "batch_size = 64\n",
    "nb_rounds = 50\n",
    "nb_episodes = 5\n",
    "filename_summary = 'summary.txt'\n",
    "alpha = 0.2 \n",
    "gamma = 0.9 \n",
    "buffer_size = 10000 # Size of replay memory D \n",
    "fixed_length = True # Fixed memory length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGenerator('../data/user_mini_data.tar.gz', '../data/music_mini_data.tar.gz')\n",
    "dg.gen_train_test(train_ratio = 0.7, seed = 42)\n",
    "dg.write_csv('train_set.csv', dg.train, nb_states = [history_length], nb_actions = [ra_length])\n",
    "dg.write_csv('test_set.csv', dg.test, nb_states = [history_length], nb_actions = [ra_length])\n",
    "data = read_file('train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # Generate embeddings\n",
    "    eg = EmbeddingsGenerator(dg.user_train, dg.data)\n",
    "    eg.train(nb_epochs = 200)\n",
    "    eg.save_embeddings('../src/embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings(read_embeddings('../src/embeddings.csv'))\n",
    "\n",
    "state_space_size = embeddings.size() * history_length\n",
    "action_space_size = embeddings.size() * ra_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sweta\\AppData\\Local\\Temp\\ipykernel_2304\\3458907209.py:74: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Sweta\\AppData\\Local\\Temp\\ipykernel_2304\\3458907209.py:60: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "WARNING:tensorflow:From c:\\Users\\Sweta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From c:\\Users\\Sweta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Episode 1/5 Time = 9s No replay\n",
      "Episode 2/5 Time = 506s Loss = 8.3892\n",
      "Episode 3/5 Time = 661s Loss = 2.7190\n",
      "Episode 4/5 Time = 664s Loss = 2.3052\n",
      "Episode 5/5 Time = 654s Loss = 1.6668\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "environment = Environment(data, embeddings, alpha, gamma, fixed_length)\n",
    "tf.reset_default_graph() # For multiple consecutive executions\n",
    "sess = tf.Session()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Initialize actor network f_θ^π and critic network Q(s, a|θ^µ) with random weights\n",
    "actor = Actor(sess, state_space_size, action_space_size, batch_size, ra_length, history_length, embeddings.size(), tau, actor_lr)\n",
    "critic = Critic(sess, state_space_size, action_space_size, history_length, embeddings.size(), tau, critic_lr)\n",
    "agent_train(sess, environment, actor, critic, embeddings, history_length, ra_length, buffer_size, batch_size, discount_factor, nb_episodes, filename_summary, nb_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_embeddings = {}\n",
    "for i, item in enumerate(embeddings.get_embedding_vector()):\n",
    "    str_item = str(item)\n",
    "    assert(str_item not in dict_embeddings)\n",
    "    dict_embeddings[str_item] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_items(state, actor, ra_length, embeddings, dict_embeddings, target = False):\n",
    "    return [dict_embeddings[str(action)]\n",
    "          for action in actor.get_recommendation_list(ra_length, np.array(state).reshape(1, -1), \n",
    "                                                      embeddings, target).reshape(ra_length, embeddings.size())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_columns = ['acousticness','beat_strength', 'bounciness', 'danceability', \n",
    "                 'energy', 'liveness', 'speechiness', 'valence']\n",
    "def test_actor(actor, test_df, all_df, embeddings, dict_embeddings, ra_length, history_length, target = False, nb_rounds = 2):\n",
    "    ratings = []\n",
    "    unknown = 0\n",
    "    diversity = []\n",
    "\n",
    "    for _ in range(nb_rounds):\n",
    "        for i in range(len(test_df)):\n",
    "            history_sample = list(test_df[i].sample(history_length)['music_id'])\n",
    "            recommendation = state_to_items(embeddings.embed(history_sample), actor, ra_length, embeddings, dict_embeddings, target)\n",
    "            features = []\n",
    "            for item in recommendation:\n",
    "                l = list(test_df[i].loc[test_df[i]['music_id']==item]['rating'])\n",
    "                if len(l) == 0:\n",
    "                    unknown += 1\n",
    "                else:\n",
    "                    ratings.append(l[0])\n",
    "\n",
    "                features.append(all_df.loc[all_df['music_id']==item][music_columns].iloc[0].tolist())\n",
    "\n",
    "            similarity = cosine_similarity(features)\n",
    "            upper_right = np.triu_indices(similarity.shape[0], k = 1)\n",
    "            diversity.append(1-np.mean(similarity[upper_right]))\n",
    "\n",
    "    return ratings, diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set\n",
    "ratings, diversity = test_actor(actor, dg.test, dg.data, embeddings, dict_embeddings, ra_length, history_length, target = False, nb_rounds = 2)\n",
    "ratings = pd.DataFrame(ratings)\n",
    "\n",
    "test_rating, baseline_rating = (1-ratings.mean())*100,(1-dg.data.rating.mean())*100\n",
    "all_ratings = [test_rating, baseline_rating]\n",
    "fig, ax = plt.subplots(figsize = (7,4))\n",
    "labels = ['RL', 'Traditional methods']\n",
    "x = np.arange(len(labels))\n",
    "results = ax.bar(x, all_ratings, width = 0.5)\n",
    "ax.set_ylabel('Rate%', fontsize = 15)\n",
    "ax.set_title('Skip rate', fontsize = 15)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
